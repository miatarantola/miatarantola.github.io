[
  {
    "objectID": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "href": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "link to perceptron code\n\n\nIn this blog post we will delve into the perceptron algorithm. I have implemented a algorithm that separates binary data. We have also conducted experiments that push the limits of our algorithm and show cases different patterns\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n\n#importing perceptron.py code and updating\nimport perceptron\nfrom perceptron import Perceptron\nimport importlib\nimportlib.reload(perceptron)\n\n<module 'perceptron' from '/Users/mtarantola@middlebury.edu/Downloads/Machine Learning/miatarantola.github.io/posts/Perceptron Blog Post/perceptron.py'>\n\n\n\n\n\nThe following function is used to update the perceptron weights:\n\n\\(\\tilde{w} ^{(t+1)} = \\tilde{w} ^ {(t)} + \\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0) \\tilde{y_i} \\tilde{x_i}\\)\n\nIn order to implement this algorithm, we must follow these steps.\n\\({1.}\\) pick a random index \\(~{i} \\in\\) n. \n\\({2. }\\) predict the label, \\(\\hat{y_i}\\), of our randomly selected data point, \\(\\tilde{x_i}\\).\n\nTo do so we calculate the dot product, \\(\\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle\\) and compare the resulting value to 0.\n\n\nif the result is greater than 0 return 1, otherwise return -1.\n\n\\({3.}\\) compute \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\)\n\nGiven our predicted label, \\(\\hat{y_i}\\) , of 1 or -1 we can multiply by \\(\\tilde{y_i}\\) to check for correctness\n\n\nIf \\(\\hat{y_i} \\tilde{y_i} <0\\) then the signs of our observed and predicted label do not match. If \\(\\hat{y_i} \\tilde{y_i} >0\\) then the signs of our predicted and observed label match\n\nSo, \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\) returns 1 if model predicted incorrectly and \\(\\tilde{y_i} \\tilde{x_i}\\) is added to the pre-existing weights.\nOtherwise the expression returns 0 for a correctly prediction and no update is performed\n\n\n\n\n\n\nnp.random.seed(8674)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = 2,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np1 = Perceptron()\n\n\np1.fit(X1,y1,100000)\n\n\nfig1 = plt.plot(p1.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np1.score(X1,y1)\n\n1.0\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig = draw_line(p1.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nFor this experiment we ran our perceptron algorithm on linearly separable data. We can see that the reuslting line is a good separator as it clearly separates our two labels. It is interesting to see that the accuracy decreases before it increases. Another result that suggests our separator is a good fit is that our algorithm stops before the maximum number of iterations allowed\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\np_features = 3\n\nX2, y2 = make_blobs(n_samples = 100,n_features=2, centers=2)\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np2 = Perceptron()\n\n\np2.fit(X2,y2,1000)\n\n\nfig3 = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X2,y2)\n\n0.5\n\n\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis experiment displays the use of our perceptron algorithm on nonlinearly separable data. As shown above, the resulting line is not a good separater. In fact, it isn’t separating any data. We can also see the inaccuracy by tracking the accuracy over iterations. We can see that the algorihtm is not converging to one set of weights and thus has not found an optimizer.\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\n\nX3, y3 = make_blobs(n_samples = 100,n_features=6, centers=2)\n\n\np3 = Perceptron()\n\n\np3.fit(X3,y3,1000)\n\n\nfig4 = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X3,y3)\n\n1.0\n\n\nYes, I do believe that my data is linearly separable because my perceptron’s accuracy converges to 1 after 18 iterations. This means that my perceptron reached 100% accuracy and did not complete upon reaching that maximum number of iterations.\n\n\n\n\nThe time complexity for a single iteration of the perceptron algorithm is O(p) because predicting the label of a single data point requires calculating the dot product of x and w. Thus, np iterates over the p features of x to calulate the dot product. All other steps of this equation invlove simple multiplication or addition which are O(1)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "A blog post relating to our talk with Timnit Gebru\n\n\n\n\n\n\nApr 16, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nA blog post relating to u\n\n\n\n\n\n\nApr 16, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA blog post investigating the bias of prediction models\n\n\n\n\n\n\nApr 2, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn blog post illustrating the key techniques of linear regression\n\n\n\n\n\n\nApr 2, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn blog post practing machine learning techniques on real life data\n\n\n\n\n\n\nMar 10, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn blog post illustrating the key techniques of gradient descent\n\n\n\n\n\n\nMar 9, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn blog post illustrating course material on perceptrons\n\n\n\n\n\n\nFeb 19, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "We are looking at the use of gradient descent for optimization and the logistic loss problem. In this assignment I implement gradient descent for logistic regression, implement stochastic gradient descent and perform several experiments that test the limits of the algorithms\nlink to gradient descent code\n\n\n\n\nimport importlib\nimport gradient_descent\nfrom gradient_descent import LogisticRegression \n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#stochastic-descent",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#stochastic-descent",
    "title": "Gradient Descent",
    "section": "Stochastic Descent",
    "text": "Stochastic Descent\n\nnp.random.seed(8674)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X2, y2, alpha = 0.1, max_epochs = 1000, batch_size = 15)\n\nThe Stochastic fit function beginns similiarly to the original gradient descent function: > - create X_ > - generate random weight vector of size features+1 > - set previous loss = infinity\nThen we have to iterate thrugh the following for the number of max epochs or until the loss converges:\n\n\nshuffle the points randomly\npick the first random k points and update the weights vector using the stochastic gradient\npick the next set of random points and repeat\nupdate the loss and score history\nreshuffle the points randomly and iterate again\n\n\n\n\nthe gradient function is the same as the original one, we are just inputting a subset of X_ and y_ into the function repeatedly\n\n\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig2 = draw_line(LR2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LR2.loss_history[-3])\nprint(LR2.loss_history[-2])\nprint(LR2.loss_history[-1])\nprint(LR2.score_history[-1])\n\n0.0033704921863074765\n0.003368276616639715\n0.0033660810616696095\n1.0\n\n\nThis is a good separator as our loss is close to 0 and our score is close to 1"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#comparing-methods",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#comparing-methods",
    "title": "Gradient Descent",
    "section": "Comparing methods",
    "text": "Comparing methods\n\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  max_epochs = 10000, \n             \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (a=0.05)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  max_epochs = 10000, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient a = .1\")\n\nLR = LogisticRegression()\nLR.fit(X2, y2, alpha = .05, max_epochs = 10000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nFor these settings, we can see that the stochastic methods are performing much better than the regular gradient descent method. All three methods have a smooth decline in loss, but the stochastic gradient seems to have had the fastest decline. I believe the the gradient methods would need more epochs to find a better solution."
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-alpha-is-too-large",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-alpha-is-too-large",
    "title": "Gradient Descent",
    "section": "Experiment: alpha is too large",
    "text": "Experiment: alpha is too large\n\nnp.random.seed(4001)\n\nn = 100\np_features = 3\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n#alpha is too large\nLR3 = LogisticRegression()\nLR3.fit(X3, y3, alpha = 200, max_epochs = 10000)\n\n#alpha is a normal value\nLR5 = LogisticRegression()\nLR5.fit(X3, y3, alpha = .01, max_epochs = 10000)\n\n\n\n#alpha = 200\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR3.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nLR3.loss_history\nLR3.w\n\narray([125.42362443, 123.04901387, -14.35961152])\n\n\n\n\n\n\n#alpha = .01\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR5.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nnum_steps1 = len(LR3.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR3.loss_history, label = \"(a=200)\")\n\nnum_steps2 = len(LR5.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR5.loss_history, label = \"a =.01\")\n\n\n\n\nlegend = plt.legend() \n\n\n\n\nWe can see that both alphas result in good separators. But, we have discovered one of the caveats of logistic regression. It can combat alphas that are too large by increasing all of the weights. So, for data that is linearly separable the model performs as good as you want it to. Hence, we will now investigate non-linearly separable data\n\nNon linearly separable\n\nnp.random.seed(8680)\n\nn = 100\np_features = 4\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1,-1),(1,1)])\n\n\n\nfig1 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR6 = LogisticRegression()\nLR6.fit(X4, y4, alpha = 100, max_epochs = 10000)\n\nLR7 = LogisticRegression()\nLR7.fit(X4, y4, alpha = 1, max_epochs = 10000)\n\n\n\nfig3 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nfig3 = draw_line(LR6.w, -2, 2)\n\n\n\n\n\nfig3 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nfig3 = draw_line(LR7.w, -2, 2)\n\n\n\n\n\n\nnum_steps1 = len(LR6.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR6.loss_history, label = \"(a=100)\")\n\n\nnum_steps1 = len(LR7.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR7.loss_history, label = \"(a=1)\")\n\n\nlegend = plt.legend() \n\n\n\n\nHere we can see that our model performs fairly well at a smaller alpha (1) as the line mostly separates the data. However, for a larger alpha the algorithm does not work well. It is unable to converge and thus results in a bad separater and the loss bounces around"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-batch-size",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-batch-size",
    "title": "Gradient Descent",
    "section": "Experiment batch size",
    "text": "Experiment batch size\n\nnp.random.seed(8680)\n\nn = 100\np_features = 4\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X4, y4, \n                  max_epochs = 10000, \n                  batch_size = 8, \n                  alpha = .1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch_size=8)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X4, y4, \n                  max_epochs = 10000, \n                  batch_size = 80, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch_size=80)\")\n\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nFor this experiment I compared a stochastic batch size of 8 to a batch size of 80. We can see that a smaller batch size allows the algorithm to converge must faster. Increasing the batch size by a factor of 8 almost increased the number of needed epochs by a factor of 10"
  },
  {
    "objectID": "posts/Classifying Palmer Penguins/Penguin Blog Post.html",
    "href": "posts/Classifying Palmer Penguins/Penguin Blog Post.html",
    "title": "Penguin Blog Post",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\",\"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head() #looking at data\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\ny_train.mean()\n\n0.95703125\n\n\n\n\n\nThe purpose of this blog post is to practice implemeting machine learning classifiers on real life data! We are looking at the plamer penguins data set collected by Dr. Kristen Gorman. Here we analyze different attributes to use in our species prediction for penguins\n\n\n\npalmer_penguins.png\n\n\n\n\n\n\nsns.relplot(data = train, x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", size = \"Body Mass (g)\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f780c3ac9d0>\n\n\n\n\n\nThe graph explores the realtionship between species, Flipper Length and Culmen Length. We can see a clear divide between the species. The Gentoo penguins seem to have the longeth flipper and culmen length and are the heaviest. Chinstrap penguins appear to have a longer culmen length but a shorter flipper length. Adelie Penguins have the shortest Flipper and Culmen length. Despite having a longer culmen length, the chinstrap penguins are about the same weight as the Adelies.\n\ntrain.groupby(\"Species\").agg(\"mean\")\n\n/var/folders/k6/ddn01bfs47n_n5b4t6fb88q80000gp/T/ipykernel_58878/1693257315.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  train.groupby(\"Species\").agg(\"mean\")\n\n\n\n\n\n\n  \n    \n      \n      Sample Number\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      73.440678\n      38.710256\n      18.365812\n      189.965812\n      3667.094017\n      8.855229\n      -25.825165\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      32.696429\n      48.719643\n      18.442857\n      195.464286\n      3717.857143\n      9.338671\n      -24.542886\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      61.930693\n      47.757000\n      15.035000\n      217.650000\n      5119.500000\n      8.240438\n      -26.168937\n    \n  \n\n\n\n\nThis tables looks at the quantitative variables averaged by species. It is interesting to see that the Adelie penguins have a deeper culmen, yet are significantly lighter than the Gentoo penguins. In general a larger flipper length correlates to a heavier body mass. However, the penguins with a smaller culmen depth have a lighter body mass.\n\n\n\nFirst we must check the base rate of our data set - If we were to guess one species for all samples, what would the highest accuracy be? To do this we must calculate the number of each species and compare this to the total number of penguins\n\nnum_penguins = y_train.shape[0]\nspecies1 = np.count_nonzero(y_train == 0)\nspecies2 = np.count_nonzero(y_train == 1)\nspecies3 = np.count_nonzero(y_train == 2)\n\nmax([species1,species2,species3])/num_penguins\n\n0.4140625\n\n\nHere we took the spcies with the largest number of samples and divided by the total number of penguins. This gives us our base rate. If we guessed one species for all of the penguins we would be correct at most 41% of the time\n\n\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nimport numpy as np \n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Sex\",\"Clutch Completion\",\"Island\",\"Region\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nbest_col= []\nbest_score = -np.inf\n\nbest_gamma=np.inf\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    for gammas in 10**(np.arange(-4, 4,dtype=float)):\n        clf = SVC(gamma = gammas) #kernel=\"linear\",probability=True)\n        scores = cross_val_score(clf, X_train[cols], y_train, cv=5)\n        if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_col= cols\n            best_gamma = gammas\nprint(best_col, best_score, best_gamma)\n        \n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.984389140271493 0.1\n\n\nThe method above chooses the best penguin attributes to use for our model as well as the best gamma, which determines the complexity of the model. To do so we iterate through the different combinations of attributes incluiding 1 qualitative and 2 quantitative variables. We then iterate through a series of gammas and analyze the accuracy of an SVC model using cross validation. The best attributes, score and gamma are recorded for testing\n\n\nbest_model = SVC(gamma = best_gamma)\n\nHere we initialize and train a final model using our initial dataset, and optimized parameters that we just discovered\n\nbest_model.fit(X_train[sorted(best_col, reverse = True)],y_train)\n\nSVC(gamma=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma=0.1)\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nbest_model.score(X_test[sorted(best_col,reverse=True)], y_test)\n\n0.9558823529411765\n\n\nWe can see that the final model predicts a penguins species, given their island, culmen length and culmen depth with 95% accuracy. This is definitely better than our base rate, However, we will now experiment with a different method to see if we can increase our results\n\n\n\n\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport numpy as np \n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Sex\",\"Clutch Completion\",\"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\"]\n\nbest_col= []\nbest_score = -np.inf\n\nmaximum=np.inf\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    for depth in range(2,25):\n        clf = DecisionTreeClassifier(max_depth = depth)\n        scores = cross_val_score(clf, X_train[cols], y_train, cv=4)\n        if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_col= cols\n            maximum = depth\nprint(best_col, best_score, maximum)\n\n        \n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.984375 4\n\n\nSimilar to the method above, we begin by iterating through all combinations of attributes. Next we use cross validation to determine the best value for maximum depth. We keep track of the best attributes, score and depth.\n\nmodel = DecisionTreeClassifier(max_depth = maximum)\nmodel.fit(X_train[sorted(best_col)],y_train)\nmodel.score(X_test[sorted(best_col)], y_test).round(3)\n\n0.985\n\n\nWe trained a final model with our optimal parameters and tested it our test data. The final model predicted the correct penguin species 98.5% of the time. This is better than the base rate as well as the SVC model. We will now dive further into the accruacy of this model\n\nWe will look at a confusion matrix to see what our accuracy loss is originating. As we can see below, the model preformed accrurate on all but one penguin. This penguin was species1 however our model predicted that it was a species 2 penguin.\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test[sorted(best_col)])\n\nconfusion_matrix(y_test, y_pred)\n\narray([[32,  1,  0],\n       [ 0, 12,  0],\n       [ 0,  0, 23]])\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n \n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(model,X_train[sorted(best_col)],y_train)\n\n\n\n\nThe plots above visualize our decision regions for the decision tree model. We can see that our model does a pretty good job at separating the different species; however, the middle plot has some samples right on the edge\n\n\n\n\nThis blog post allowed me to practice a machine learning workflow and get hands on experience implementing a classifier with real data. I really enjoyed exploring the different classification models and optimization parameters. I also enjoyed working with real life data, I can really see how this will help me in my future!"
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#analytic-approach",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#analytic-approach",
    "title": "Linear Regression",
    "section": "Analytic Approach",
    "text": "Analytic Approach\nThe analytic apporach makes use of the definition of convergence during gradient descent. At the minimum, \\(\\nabla L(w) = 0\\). Setting this equal to our expression for \\(\\nabla L(w)\\) gives \\(0 = X^{T}(X~\\hat{w} -y)\\). Solving for \\(\\hat{w}\\) gives \\(\\hat{w} = (X^{T}X)^{-1}X^{T}y\\), which can bed used to directly solve for \\(\\hat{w}\\) assuming X is padded. Now lets test the approach and generate some data\n\np_features = 1.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.13\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow that we have our data, let’s train a model with the corresponding training data. For the first step of the function, I padded X_train the plugged it into the equation\n\nLR_analytic = LinearRegression()\n\n\nLR_analytic.fit_analytic(X_train,y_train)\n\nLet’s plot our new model\n\nplt.scatter(X_train,y_train)\nplt.plot(X_train,pad(X_train)@LR_analytic.w, color=\"black\")\n\n\n\n\nWe can see that this model produces a pretty good fit forour data. It roughly follows the positive linear pattern of our data, but let’s look at the accuracy to get a better idea of performance\n\nprint(f\"Training score = {LR_analytic.score(pad(X_train),y_train).round(4)}\")\nprint(f\"Validation score = {LR_analytic.score(pad(X_val),y_val).round(4)}\")\n\nTraining score = 0.5225\nValidation score = 0.5491\n\n\nOur accuracies are pretty good considering the coefficient of determination is always no larger than 1 and can be arbitrarily negative for very bad models."
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#gradient-descent-approach",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#gradient-descent-approach",
    "title": "Linear Regression",
    "section": "Gradient Descent Approach",
    "text": "Gradient Descent Approach\nNow let’s look at the gradient descent approach. This approach makes use of the gradient function \\(\\nabla L(w) = 2X^{T}(Xw-y)\\). We use this during the repeadted iteration of \\[w^{(t+1)} \\leftarrow w^{(t)} - 2\\alpha X^{T}(Xw^{(t)} -y)\\] we iterate until we hit the max number of iterations or until the gradient =0. We also track the score over the iterations. Let’s use the same data to compare our results\n\nLR_gradient = LinearRegression()\nLR_gradient.fit_grad(X_train,y_train)\n\n\nplt.scatter(X_train,y_train)\nplt.plot(X_train, pad(X_train)@LR_gradient.w, color=\"black\")\n\n\n\n\nonce again we can see that model does a good job predicting as the resulting line follows the positive linear pattern of the training data. Let’s look at the accuracy\n\nprint(f\"Training score = {LR_gradient.score(pad(X_train),y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(pad(X_val),y_val).round(4)}\")\n\nTraining score = 0.5225\nValidation score = 0.5491\n\n\nThese scores are once again pretty good considering the scores max is 1 and can be arbitrarily negative for bad models. It also a good sign that the scores are the same as the analytic approach! Now, let’s look at the score progression.\n\nplt.plot(LR_gradient.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs expected, we see that our model started off with low accuracy, but quickly achieve a higher accuracy!"
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#experiments",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\np_features_exp = [2,50,100,150,170,180,190,199]\n\nn_train = 200\nn_val = 200\nnoise = 0.1\ntraining_scores=[]\nval_scores=[]\n\n# create some data\n\nfor p in p_features_exp:\n    X_train_exp, y_train_exp, X_val_exp, y_val_exp = LR_data(n_train, n_val, p, noise)\n    # print( X_train_exp[:5])\n    LR_exp1 = LinearRegression()\n    LR_exp1.fit_analytic(X_train_exp,y_train_exp)\n    training_scores.append(LR_exp1.score(pad(X_train_exp),y_train_exp).round(4))\n    val_scores.append(LR_exp1.score(pad(X_val_exp),y_val_exp).round(4))\n\n\n\n\nfig1 = plt.scatter(p_features_exp,training_scores)\nplt.scatter(p_features_exp,val_scores)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Score\")\nplt.legend([\"Training\",\"Validation\"])\n\n<matplotlib.legend.Legend at 0x7fa3f7c56820>\n\n\n\n\n\nThis trend makes sense to me. Training a model is fairly easy and thus it is fairly easy to achieve a high accurcay/score for your training data. So, a pretty flat but high training accuracy was expected. The validation score is what matters as it shows how the model will perform on new data. The model starts of with a realtivelhy low vlaidation accuracy and it increases until the number of features is very similar to the number of data points. This is due to over fitting. As more features are added to our data, the model becomes increasingly complex, having to account for more and more properties. While it is complex, the training accruacy is still high as the model was trained on the data that it is evaluating. As we introduce new data (val data) the model has been over fitting the data (increasingly complex to fit the training data), so the model is so complex and specific to the training data that it is not performing well on the validation data."
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#lasso-regularization",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb-Copy1.html#lasso-regularization",
    "title": "Linear Regression",
    "section": "LASSO REGULARIZATION",
    "text": "LASSO REGULARIZATION\n\n#importing from scikit learn\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\np_features_exp2 = [2,50,100,150,170,180,190,199, 210, 220]\n\nn_train2 = 200\nn_val2 = 200\nnoise = 0.1\ntraining_scores2=[]\nval_scores2=[]\n\n# create some data\n\nfor p in p_features_exp2:\n    X_train_exp2, y_train_exp2, X_val_exp2, y_val_exp2 = LR_data(n_train2, n_val2, p, noise)\n\n    L.fit(X_train_exp2,y_train_exp2)\n    training_scores2.append(L.score(X_train_exp2,y_train_exp2).round(4))\n    val_scores2.append(L.score(X_val_exp2,y_val_exp2).round(4))\n\n\nfig2 = plt.scatter(p_features_exp2,training_scores2)\nplt.scatter(p_features_exp2,val_scores2)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Score\")\nplt.legend([\"Training\",\"Validation\"])\n\n<matplotlib.legend.Legend at 0x7fa3f8051be0>\n\n\n\n\n\nThe training scores look to be about the same as the scores generated from the linear regression model. That being relatively stable and high. The validation scores show a bit of a different pattern than the scores from purely LR model. In the LR model scores, the val scores remained high until just before p = n. The LASSO regularization model shows more of gradual decrease in validation accuracy. The model seems to hit its validation accuracy peak around p=n/2 and starts a steady decline for larger Ps. However, the validation accuracy for p=n-1 is larger with the LASSO model in comparison to purely LR"
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#analytic-approach",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#analytic-approach",
    "title": "Linear Regression",
    "section": "Analytic Approach",
    "text": "Analytic Approach\nThe analytic apporach makes use of the definition of convergence during gradient descent. At the minimum, \\(\\nabla L(w) = 0\\). Setting this equal to our expression for \\(\\nabla L(w)\\) gives \\(0 = X^{T}(X~\\hat{w} -y)\\). Solving for \\(\\hat{w}\\) gives \\(\\hat{w} = (X^{T}X)^{-1}X^{T}y\\), which can bed used to directly solve for \\(\\hat{w}\\) assuming X is padded. Now lets test the approach and generate some data\n\np_features = 1.\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.13\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\nNow that we have our data, let’s train a model with the corresponding training data. For the first step of the function, I padded X_train the plugged it into the equation\n\nLR_analytic = LinearRegression()\n\n\nLR_analytic.fit_analytic(X_train,y_train)\n\nLet’s plot our new model\n\nplt.scatter(X_train,y_train)\nplt.plot(X_train,pad(X_train)@LR_analytic.w, color=\"black\")\n\n\n\n\nWe can see that this model produces a pretty good fit forour data. It roughly follows the positive linear pattern of our data, but let’s look at the scores to get a better idea of performance\n\nprint(f\"Training score = {LR_analytic.score(pad(X_train),y_train).round(4)}\")\nprint(f\"Validation score = {LR_analytic.score(pad(X_val),y_val).round(4)}\")\n\nTraining score = 0.5225\nValidation score = 0.5491\n\n\nOur accuracies are pretty good considering the coefficient of determination is always no larger than 1 and can be arbitrarily negative for very bad models."
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#gradient-descent-approach",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#gradient-descent-approach",
    "title": "Linear Regression",
    "section": "Gradient Descent Approach",
    "text": "Gradient Descent Approach\nNow let’s look at the gradient descent approach. This approach makes use of the gradient function \\(\\nabla L(w) = 2X^{T}(Xw-y)\\). We use this during the repeadted iteration of \\[w^{(t+1)} \\leftarrow w^{(t)} - 2\\alpha X^{T}(Xw^{(t)} -y)\\] we iterate until we hit the max number of iterations or until the gradient =0. We also track the score over the iterations. Let’s use the same data to compare our results\n\nLR_gradient = LinearRegression()\nLR_gradient.fit_grad(X_train,y_train)\n\n\nplt.scatter(X_train,y_train)\nplt.plot(X_train, pad(X_train)@LR_gradient.w, color=\"black\")\n\n\n\n\nonce again we can see that model does a good job predicting as the resulting line follows the positive linear pattern of the training data. Let’s look at the scores\n\nprint(f\"Training score = {LR_gradient.score(pad(X_train),y_train).round(4)}\")\nprint(f\"Validation score = {LR_gradient.score(pad(X_val),y_val).round(4)}\")\n\nTraining score = 0.5225\nValidation score = 0.5491\n\n\nThese scores are once again pretty good considering the scores max is 1 and can be arbitrarily negative for bad models. It also a good sign that the scores are the same as the analytic approach! Now, let’s look at the score progression.\n\nplt.plot(LR_gradient.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")\n\n\n\n\nAs expected, we see that our model started off with low accuracy, but quickly achieve a higher accuracy! This is to be expected as linear regression models often do well just after one iteration."
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#experiments",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#experiments",
    "title": "Linear Regression",
    "section": "Experiments",
    "text": "Experiments\n\np_features_exp = [2,50,100,150,170,180,190,199]\n\nn_train = 200\nn_val = 200\nnoise = 0.1\ntraining_scores=[]\nval_scores=[]\n\n# create some data\n\nfor p in p_features_exp:\n    X_train_exp, y_train_exp, X_val_exp, y_val_exp = LR_data(n_train, n_val, p, noise)\n    # print( X_train_exp[:5])\n    LR_exp1 = LinearRegression()\n    LR_exp1.fit_analytic(X_train_exp,y_train_exp)\n    training_scores.append(LR_exp1.score(pad(X_train_exp),y_train_exp).round(4))\n    val_scores.append(LR_exp1.score(pad(X_val_exp),y_val_exp).round(4))\n\n\n\n\nfig1 = plt.scatter(p_features_exp,training_scores)\nplt.scatter(p_features_exp,val_scores)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Score\")\nplt.legend([\"Training\",\"Validation\"])\n\n<matplotlib.legend.Legend at 0x7fa3f7c56820>\n\n\n\n\n\nThis trend makes sense to me. Training a model is fairly easy and thus it is fairly easy to achieve a high accurcay/score for your training data. So, a pretty flat but high training accuracy was expected. The validation score is what matters as it shows how the model will perform on new data. The model starts of with a realtivelhy low vlaidation accuracy and it increases until the number of features is very similar to the number of data points. This is due to over fitting. As more features are added to our data, the model becomes increasingly complex, having to account for more and more properties. While it is complex, the training accruacy is still high as the model was trained on the data that it is evaluating. As we introduce new data (val data) the model has been over fitting the data (increasingly complex to fit the training data), so the model is so complex and specific to the training data that it is not performing well on the validation data."
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#lasso-regularization",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html#lasso-regularization",
    "title": "Linear Regression",
    "section": "LASSO REGULARIZATION",
    "text": "LASSO REGULARIZATION\n\n#importing from scikit learn\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\n\np_features_exp2 = [2,50,100,150,170,180,190,199, 210, 220]\n\nn_train2 = 200\nn_val2 = 200\nnoise = 0.1\ntraining_scores2=[]\nval_scores2=[]\n\n# create some data\n\nfor p in p_features_exp2:\n    X_train_exp2, y_train_exp2, X_val_exp2, y_val_exp2 = LR_data(n_train2, n_val2, p, noise)\n\n    L.fit(X_train_exp2,y_train_exp2)\n    training_scores2.append(L.score(X_train_exp2,y_train_exp2).round(4))\n    val_scores2.append(L.score(X_val_exp2,y_val_exp2).round(4))\n\n\nfig2 = plt.scatter(p_features_exp2,training_scores2)\nplt.scatter(p_features_exp2,val_scores2)\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Score\")\nplt.legend([\"Training\",\"Validation\"])\n\n<matplotlib.legend.Legend at 0x7fa3f8051be0>\n\n\n\n\n\nThe training scores look to be about the same as the scores generated from the linear regression model. That being relatively stable and high. The validation scores show a bit of a different pattern than the scores from purely LR model. In the LR model scores, the val scores remained high until just before p = n. The LASSO regularization model shows more of gradual decrease in validation accuracy. The model seems to hit its validation accuracy peak around p=n/2 and starts a steady decline for larger Ps. However, the validation accuracy for p=n-1 is larger with the LASSO model in comparison to purely LR"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html",
    "href": "posts/Auditing Allocative Bias/Untitled1.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blog post I will implement a machine learning model that predicts whether an individual’s income is over 50k on the basis of demographics excluding sex. I will also audit for gender bias"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#imports",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#imports",
    "title": "Auditing Allocative Bias",
    "section": "Imports",
    "text": "Imports\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#building-data",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#building-data",
    "title": "Auditing Allocative Bias",
    "section": "Building Data",
    "text": "Building Data\nFor this model we will be looking at PUMS data for the state of New Jersey. We also need to change the PINCP column values. Currently, the values represent the total income of the individual. Now we need it to represent 0 if the individual’s income was atleast $50,000 and 0 if it was less.\n\nSTATE = \"NJ\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data.loc[acs_data.PINCP <=50000,\"PINCP\"]=0\nacs_data.loc[acs_data.PINCP >50000,\"PINCP\"]=1\n\n# acs_data.head()\n\nThis data set includes many features/ columns so we will be narrowing them down to a smaller subset of feeatures to consider\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX',\"PINCP\", 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      PINCP\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      23\n      21.0\n      5\n      17\n      2\n      NaN\n      5\n      2.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      6\n      6.0\n    \n    \n      1\n      51\n      20.0\n      4\n      17\n      2\n      NaN\n      4\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      1.0\n    \n    \n      2\n      69\n      19.0\n      3\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      6.0\n    \n    \n      3\n      18\n      16.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      9\n      6.0\n    \n    \n      4\n      89\n      19.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      1\n      6.0\n    \n  \n\n\n\n\nsubsetting the features:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nNow we construct a basic problem to express our wish to use these features to predict if income (PINCP) >=$50,000 , using the race RAC1P as the group label\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\nThe result is a features matrix, a label vector and a group label vector\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(88586, 16)\n(88586,)\n(88586,)\n\n\nWe should perform a train-test split to split our data into training and testing subsets\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#basic-descriptives",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#basic-descriptives",
    "title": "Auditing Allocative Bias",
    "section": "Basic Descriptives",
    "text": "Basic Descriptives\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      RAC1P\n      ESR\n      group\n      label\n    \n  \n  \n    \n      0\n      31.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      2.0\n      3.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      False\n    \n    \n      1\n      40.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      6.0\n      6.0\n      2\n      False\n    \n    \n      2\n      40.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      6.0\n      1.0\n      1\n      True\n    \n    \n      3\n      34.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      True\n    \n    \n      4\n      75.0\n      20.0\n      5.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      6.0\n      2\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      70863\n      66.0\n      21.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      False\n    \n    \n      70864\n      32.0\n      16.0\n      5.0\n      2.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n    \n    \n      70865\n      54.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      True\n    \n    \n      70866\n      42.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n    \n    \n      70867\n      38.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1.0\n      2\n      False\n    \n  \n\n70868 rows × 18 columns\n\n\n\nThere are 70868 individuals in the data. Now lets look at what proportion have the target label equal to 1\n\n(df[\"label\"]==1).mean()\n\n0.29835750973641134\n\n\n31.2% of the data has a target label of 1 meaning 31.2% of the individuals had a total income of at least $50,000. Now, let’s check how many people are in each of the groups. The distribution is as follows:\n\ndf[\"group\"].value_counts()\n\n2    36491\n1    34377\nName: group, dtype: int64\n\n\nNow we will look at what proportion of individuals have the target label equal to 1. The distribution values are below\n\ndf.groupby([\"group\"])[\"label\"].mean()\n\ngroup\n1    0.363237\n2    0.237237\nName: label, dtype: float64\n\n\nNow we will look at intersectional trends\n\nmeans = df.groupby([\"group\",\"RAC1P\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"RAC1P\")\n\n\n\n\nIt seems like white and asians are the groups with the highest proportion of individuals that have a target label of 1 within both genders\n\nmeans = df.groupby([\"group\",\"DIS\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"DIS\")\n\n\n\n\nFor the both groups, there is a correlation between an income over $50,000 and disability status"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#training-a-model",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#training-a-model",
    "title": "Auditing Allocative Bias",
    "section": "Training a model",
    "text": "Training a model\nWe will build our model using a decision tree approach. Let’s use cross validation to determine the best depth.\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nbest_score = -np.inf\nbest_depth = 0\n\nfor depth in range(2,10):\n    pipeline = make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth = depth))\n    scores = cross_val_score(pipeline,X_train,y_train,cv = 6)\n    if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_depth = depth\nprint(best_depth,best_score)\n    \n\n8 0.8248151617554692\n\n\nNow, let’s build our final model\n\nmodel = make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth=8))\nmodel.fit(X_train,y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=8))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=8))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=8)\n\n\nNow we can generate predictions:\n\ny_hat = model.predict(X_test)\n\n\n\nOverall Metrics\n\nprint(\"The overall accuracy of this model is: \" + str((y_hat==y_test).mean().round(4)))\n\nThe overall accuracy of this model is: 0.822\n\n\n\nTP = len((np.where((y_hat==1) & (y_test==1))[0]))\nFP = len((np.where((y_hat==1) & (y_test==0))[0]))\nPPV = round((TP)/(TP+FP),4)\nprint(\"The positive predictive value of this model is: \" + str(PPV))\n\nThe positive predictive value of this model is: 0.7092\n\n\n\nconf_matrix = confusion_matrix(y_test,y_hat, normalize = 'true')\nprint(\"FPR = \" + str(conf_matrix[0][1]))\nprint(\"FNR = \" + str(conf_matrix[1][0])) \n\nFPR = 0.11845065895210544\nFNR = 0.31835419036784224"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#by-group-measures",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By Group Measures",
    "text": "By Group Measures\n\nprint(\"The accuracy for male individuals is \" + str((y_hat==y_test)[group_test==1].mean().round(4)))\nprint(\"The accuracy for female individuals is \" + str((y_hat==y_test)[group_test==2].mean().round(4)))\n\nThe accuracy for male individuals is 0.8183\nThe accuracy for female individuals is 0.8256\n\n\n\nconf_matrix_male_norm = confusion_matrix(y_test[group_test==1],y_hat[group_test==1], normalize = 'true')\nprint(\"FPR for male individuals = \" + str(conf_matrix_male_norm[0][1]))\nprint(\"FNR for male individuals = \" + str(conf_matrix_male_norm[1][0])) \n\nconf_matrix_female_norm = confusion_matrix(y_test[group_test==2],y_hat[group_test==2], normalize = 'true')\nprint(\"FPR for female individuals = \" + str(conf_matrix_female_norm[0][1]))\nprint(\"FNR for female individuals = \" + str(conf_matrix_female_norm[1][0])) \n\nFPR for male individuals = 0.1007483117357182\nFNR for male individuals = 0.32504038772213245\nFPR for female individuals = 0.1323761665470208\nFNR for female individuals = 0.3088572739788894"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled1.html#bias-measures",
    "href": "posts/Auditing Allocative Bias/Untitled1.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nCalibration means that the fraction of predicted people to have an income >= $$$50,000 who actually had an income >=$50,000 is the same across all groups\n\nconf_matrix_male = confusion_matrix(y_test[group_test==1],y_hat[group_test==1])\nmale_calibration = confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0][0]/(confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0].sum())\nconf_matrix_female = confusion_matrix(y_test[group_test==2],y_hat[group_test==2])\nfemale_calibration = confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0][0]/(confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0].sum())\n\nprint(\"The fraction of males predicted to have an income >=$50000 who actually had an income of >=$50000 is \" + str(male_calibration.round(4)))\nprint(\"The fraction offe males predicted to have an income >=$50000 who actually had an income of >=$50000 is \" + str(female_calibration.round(4)))\n\n\nThe fraction of males predicted to have an income >=$50000 who actually had an income of >=$50000 is 0.8993\nThe fraction offe males predicted to have an income >=$50000 who actually had an income of >=$50000 is 0.8676\n\n\nI would say that the model is calibrated as the proportions for each group are roughly the same.\nA model satisfies error rate balance if the false positive and false negative rates are equal across groups. Looking at the previously calculated FPR anf FNR, I would say that this model satidfies error rate balance. The FPR for both groups is rounghly 12% and the FNR for both groups is roughly 31%.\nA model satisifes statistical parity if the proportion of individuals classified as having an income >=$50000 is the same for each group.\n\nprint(\"The proportion of males classified as having an income greater than $50,000 is \" + str((confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0].sum())/34377))\nprint(\"The proportion of females classified as having an income greater than $50,000 is \" + str((confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0].sum())/36491))\n\nThe proportion of males classified as having an income greater than $50,000 is 0.1593798179015039\nThe proportion of females classified as having an income greater than $50,000 is 0.19086898139267217\n\n\nAgain, these values are roughly the same, so I would say that this model satisfies statistical parity.\n\nCompanies like banks or credit card companies could benefit from this model. Banks often give out loans and need to asses how much money is a safe amount to give to their clients. Knowing their clients’ incomes would be very helpful in assesing the risks of pursuing a loan. Credit card companies follow a similar process when determining to accept a clients’ credit card request.\nI think it is interesting that this model gives companies/organizations that don’t usually have access to your income more insight into your financials. Job employers usually don’t have access to your current salary/wage, but now this algorithm will give them more insight and could potentially help them in deciding a new salary offer. Places like country clubs, whos reputation is encompassed by wealth, could use this kind of algorithm.\nBased on my bias audit, I don’t think that my model displays problematic bias. The model satisifies the calibration, error rate and statistical parity tests. So, there is no obvious bias, but as always more investigating is also helpful.\nIn my opion the accuracy of this model is not high enoughly to be deployed. The accuracy of the model is about 80% meaning it predicts the wrong out come 20% of the time. To put this into perspective, this model is incorrect for every 5th person. To help remedy this, I would suggest either collecting more data or reevaluating the parameters/factors involved"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled.html",
    "href": "posts/Auditing Allocative Bias/Untitled.html",
    "title": "Penguin Blog Post",
    "section": "",
    "text": "In this blog post I will implement a machine learning model that predicts whether an individual’s income is over 50k on the basis of demographics excluding sex. I will also audit for gender bias"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled.html#imports",
    "href": "posts/Auditing Allocative Bias/Untitled.html#imports",
    "title": "Penguin Blog Post",
    "section": "Imports",
    "text": "Imports\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Untitled.html#building-a-model",
    "href": "posts/Auditing Allocative Bias/Untitled.html#building-a-model",
    "title": "Penguin Blog Post",
    "section": "Building a Model",
    "text": "Building a Model\nFor this model we will be looking at PUMS data for the state of New Jersey. We also need to change the PINCP column values. Currently, the values represent the total income of the individual. Now we need it to represent 0 if the individual’s income was atleast $50,000 and 0 if it was less.\n\nSTATE = \"NJ\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data.loc[acs_data.PINCP <50000,\"PINCP\"]=0\nacs_data.loc[acs_data.PINCP >=50000,\"PINCP\"]=1\n\n# acs_data.head()\n\nThis data set includes many features/ columns so we will be narrowing them down to a smaller subset of feeatures to consider\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX',\"PINCP\", 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      PINCP\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      23\n      21.0\n      5\n      17\n      2\n      NaN\n      5\n      2.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      6\n      6.0\n    \n    \n      1\n      51\n      20.0\n      4\n      17\n      2\n      NaN\n      4\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      1.0\n    \n    \n      2\n      69\n      19.0\n      3\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      6.0\n    \n    \n      3\n      18\n      16.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      9\n      6.0\n    \n    \n      4\n      89\n      19.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      1\n      6.0\n    \n  \n\n\n\n\nsubsetting the features:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we construct a basic problem to express our wish to use these features to predict if income (PINCP) >=$50,000 , using the race RAC1P as the group label\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\nThe result is a features matrix, a label vector and a group label vector\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(88586, 16)\n(88586,)\n(88586,)\n\n\nWe should perform a train-test split to split our data into training and testing subsets\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nNow we build our model using\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nmodel = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(gamma='auto'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('svc', SVC(gamma='auto'))])StandardScalerStandardScaler()SVCSVC(gamma='auto')\n\n\nWe can now generate predictions:\n\ny_hat = model.predict(X_test)\n\nThe overall accuracy in predicting whether someone had an income of at least $50,000 is:\n\n(y_hat == y_test).mean()\n\n1.0\n\n\nThe accuracy for white individuals is\n\n(y_hat == y_test)[group_test ==1].mean()\n\n1.0\n\n\n\n(y_hat == y_test)[group_test == 2].mean()\n\n1.0\n\n\n\nBasic descriptives\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      PINCP\n      group\n      label\n    \n  \n  \n    \n      0\n      31.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      2.0\n      3.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.0\n      1\n      False\n    \n    \n      1\n      40.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.0\n      6\n      False\n    \n    \n      2\n      40.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      6\n      True\n    \n    \n      3\n      34.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1\n      True\n    \n    \n      4\n      75.0\n      20.0\n      5.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.0\n      1\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      70863\n      66.0\n      21.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.0\n      1\n      False\n    \n    \n      70864\n      32.0\n      16.0\n      5.0\n      2.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      0.0\n      1\n      False\n    \n    \n      70865\n      54.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      True\n    \n    \n      70866\n      42.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      0.0\n      1\n      False\n    \n    \n      70867\n      38.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      0.0\n      2\n      False\n    \n  \n\n70868 rows × 18 columns\n\n\n\nThere are 70868 individuals in the data. Now lets look at what proportion have the target label equal to 1\n\n(df[\"label\"]==1).mean()\n\n0.3120731500818423\n\n\n31.2% of the data has a target label of 1 meaning 31.2% of the individuals had a total income of at least $50,000. Now, let’s check how many people are in each of the groups. The distribution is as follows:\n\ndf[\"group\"].value_counts()\n\n1    51004\n2     7419\n6     7230\n8     3233\n9     1780\n3      114\n5       62\n7       24\n4        2\nName: group, dtype: int64\n\n\nNow we will look at what proportion of individuals have the target label equal to 1. The distribution values are below\n\ndf.groupby([\"group\"])[\"label\"].mean()\n\ngroup\n1    0.343228\n2    0.196253\n3    0.201754\n4    0.000000\n5    0.177419\n6    0.332642\n7    0.166667\n8    0.121250\n9    0.179213\nName: label, dtype: float64\n\n\nNow we will look at intersectional trends\n\nmeans = df.groupby([\"group\",\"SEX\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\n\n\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"SEX\")\n\n\n\n\nThere doesn’t look seem to be any trends between sex and group.\n\nmeans = df.groupby([\"group\",\"DIS\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\n\n\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"DIS\")\n\n\n\n\nFor the most groups, there is a correlation between an income over $50,000 and disability status"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "In this blog post I will implement a machine learning model that predicts whether an individual’s income is over 50k on the basis of demographics excluding sex. I will also audit for gender bias"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#imports",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#imports",
    "title": "Auditing Allocative Bias",
    "section": "Imports",
    "text": "Imports\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#building-data",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#building-data",
    "title": "Auditing Allocative Bias",
    "section": "Building Data",
    "text": "Building Data\nFor this model we will be looking at PUMS data for the state of New Jersey. We also need to change the PINCP column values. Currently, the values represent the total income of the individual. Now we need it to represent 0 if the individual’s income was atleast $50,000 and 0 if it was less.\n\nSTATE = \"NJ\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\nacs_data.loc[acs_data.PINCP <=50000,\"PINCP\"]=0\nacs_data.loc[acs_data.PINCP >50000,\"PINCP\"]=1\n\n# acs_data.head()\n\nThis data set includes many features/ columns so we will be narrowing them down to a smaller subset of feeatures to consider\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX',\"PINCP\", 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      PINCP\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      23\n      21.0\n      5\n      17\n      2\n      NaN\n      5\n      2.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      6\n      6.0\n    \n    \n      1\n      51\n      20.0\n      4\n      17\n      2\n      NaN\n      4\n      1.0\n      4.0\n      1\n      2\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      1.0\n    \n    \n      2\n      69\n      19.0\n      3\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      0.0\n      1\n      6.0\n    \n    \n      3\n      18\n      16.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      2\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      9\n      6.0\n    \n    \n      4\n      89\n      19.0\n      2\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      4\n      1\n      2\n      2\n      1.0\n      2\n      0.0\n      1\n      6.0\n    \n  \n\n\n\n\nsubsetting the features:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"PINCP\", \"SEX\"]]\n\nNow we construct a basic problem to express our wish to use these features to predict if income (PINCP) >=$50,000 , using the race RAC1P as the group label\n\nIncomeProblem = BasicProblem(\n    features=features_to_use,\n    target='PINCP',\n    target_transform=lambda x: x == 1,\n    group='SEX',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = IncomeProblem.df_to_numpy(acs_data)\n\nThe result is a features matrix, a label vector and a group label vector\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(88586, 16)\n(88586,)\n(88586,)\n\n\nWe should perform a train-test split to split our data into training and testing subsets\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#basic-descriptives",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#basic-descriptives",
    "title": "Auditing Allocative Bias",
    "section": "Basic Descriptives",
    "text": "Basic Descriptives\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\ndf\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      RAC1P\n      ESR\n      group\n      label\n    \n  \n  \n    \n      0\n      31.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      2.0\n      3.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      False\n    \n    \n      1\n      40.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      6.0\n      6.0\n      2\n      False\n    \n    \n      2\n      40.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      5.0\n      1.0\n      4.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      6.0\n      1.0\n      1\n      True\n    \n    \n      3\n      34.0\n      21.0\n      1.0\n      1.0\n      2.0\n      0.0\n      1.0\n      3.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      True\n    \n    \n      4\n      75.0\n      20.0\n      5.0\n      0.0\n      1.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      6.0\n      2\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      70863\n      66.0\n      21.0\n      4.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      2\n      False\n    \n    \n      70864\n      32.0\n      16.0\n      5.0\n      2.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      2.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n    \n    \n      70865\n      54.0\n      22.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      True\n    \n    \n      70866\n      42.0\n      19.0\n      1.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      1.0\n      1.0\n      1\n      False\n    \n    \n      70867\n      38.0\n      14.0\n      5.0\n      0.0\n      2.0\n      0.0\n      1.0\n      1.0\n      4.0\n      1.0\n      1.0\n      2.0\n      2.0\n      2.0\n      2.0\n      1.0\n      2\n      False\n    \n  \n\n70868 rows × 18 columns\n\n\n\nThere are 70868 individuals in the data. Now lets look at what proportion have the target label equal to 1\n\n(df[\"label\"]==1).mean()\n\n0.29835750973641134\n\n\n31.2% of the data has a target label of 1 meaning 31.2% of the individuals had a total income of at least $50,000. Now, let’s check how many people are in each of the groups. The distribution is as follows:\n\ndf[\"group\"].value_counts()\n\n2    36491\n1    34377\nName: group, dtype: int64\n\n\nNow we will look at what proportion of individuals have the target label equal to 1. The distribution values are below\n\ndf.groupby([\"group\"])[\"label\"].mean()\n\ngroup\n1    0.363237\n2    0.237237\nName: label, dtype: float64\n\n\nNow we will look at intersectional trends\n\nmeans = df.groupby([\"group\",\"RAC1P\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"RAC1P\")\n\n\n\n\n\n\n\nGroup\nRAC1P\n\n\n\n\n1: male\n1: White alone\n\n\n2: female\n2: Black or African American alone\n\n\n\n3: American Indian alone\n\n\n\n4: Alaska Native alone\n\n\n\n5: American Indian, Alaska Native alone\n\n\n\n6: Asian alone\n\n\n\n7: Native Hawaiian and Other Pacific Islander alone\n\n\n\n8: Some other race alone\n\n\n\n9: Two or More Races\n\n\n\nWe are looking to see the breakdown of individuals who have a target label of 1 (meaning they have an income >=$50,000) given their race and gender. We can use the mean as our metric because our target label is binary. So, the mean is the proportion of labels with the value 1. It seems like white and asians are the groups with the highest proportion of individuals that have a target label of 1 within both genders\n\nmeans = df.groupby([\"group\",\"DIS\"])[\"label\"].mean().reset_index(name = \"proportion of individuals that have target label of 1\")\np = sns.barplot(data = means, x = \"group\", y = \"proportion of individuals that have target label of 1\", hue = \"DIS\")\n\n\n\n\nFor the both groups, there is a correlation between an income over $50,000 and disability status"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#training-a-model",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#training-a-model",
    "title": "Auditing Allocative Bias",
    "section": "Training a model",
    "text": "Training a model\nWe will build our model using a decision tree approach. Let’s use cross validation to determine the best depth.\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nbest_score = -np.inf\nbest_depth = 0\n\nfor depth in range(2,10):\n    pipeline = make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth = depth))\n    scores = cross_val_score(pipeline,X_train,y_train,cv = 6)\n    if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_depth = depth\nprint(best_depth,best_score)\n    \n\n8 0.8248151617554692\n\n\nNow, let’s build our final model\n\nmodel = make_pipeline(StandardScaler(),DecisionTreeClassifier(max_depth=8))\nmodel.fit(X_train,y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=8))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=8))])StandardScalerStandardScaler()DecisionTreeClassifierDecisionTreeClassifier(max_depth=8)\n\n\nNow we can generate predictions:\n\ny_hat = model.predict(X_test)\n\n\n\nOverall Metrics\nThe overall accuracy of the model represents the percentage of times that it will correctly predict the target label. So, we are comparing the correct labels to the target labels and taking the mean. Because our label is binary we can take the mean of our equivalence array as the number of times 1 appear (signifying the labels match) is the mean\n\nprint(\"The overall accuracy of this model is: \" + str((y_hat==y_test).mean().round(4)))\n\nThe overall accuracy of this model is: 0.822\n\n\nTo find the PPV of our model we first need to calculate the number of true positives and false positives. To do so, we compare the predicted labels to the test labels. If the predicted label is 1 and the acutal label is 1, then that is a true positive. If the predicted label is 1 and the true label is 0 that is a false positive.\n\nTP = len((np.where((y_hat==1) & (y_test==1))[0]))\nFP = len((np.where((y_hat==1) & (y_test==0))[0]))\nPPV = round((TP)/(TP+FP),4)\nprint(\"The positive predictive value of this model is: \" + str(PPV))\n\nThe positive predictive value of this model is: 0.7092\n\n\nThe FPR represents the rate at which a test label of 0 is incorrectly predicted to be 1. THE FNR represents the rate at which a test label of 1 is incorrectly predicted to be 0.\n\nconf_matrix = confusion_matrix(y_test,y_hat, normalize = 'true')\nprint(\"FPR = \" + str(conf_matrix[0][1]))\nprint(\"FNR = \" + str(conf_matrix[1][0])) \n\nFPR = 0.11845065895210544\nFNR = 0.31835419036784224"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#by-group-measures",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#by-group-measures",
    "title": "Auditing Allocative Bias",
    "section": "By Group Measures",
    "text": "By Group Measures\nNow we will compare the accuracy of our model for individual groups. So we filter by gender\n\nprint(\"The accuracy for male individuals is \" + str((y_hat==y_test)[group_test==1].mean().round(4)))\nprint(\"The accuracy for female individuals is \" + str((y_hat==y_test)[group_test==2].mean().round(4)))\n\nThe accuracy for male individuals is 0.8183\nThe accuracy for female individuals is 0.8256\n\n\nNow we will copmare some of the accuracy metrics such as the false positive rate and the false negative rate. They are about equal for both genders which is generally a good sign\n\nconf_matrix_male_norm = confusion_matrix(y_test[group_test==1],y_hat[group_test==1], normalize = 'true')\nprint(\"FPR for male individuals = \" + str(conf_matrix_male_norm[0][1]))\nprint(\"FNR for male individuals = \" + str(conf_matrix_male_norm[1][0])) \n\nconf_matrix_female_norm = confusion_matrix(y_test[group_test==2],y_hat[group_test==2], normalize = 'true')\nprint(\"FPR for female individuals = \" + str(conf_matrix_female_norm[0][1]))\nprint(\"FNR for female individuals = \" + str(conf_matrix_female_norm[1][0])) \n\nFPR for male individuals = 0.1007483117357182\nFNR for male individuals = 0.32504038772213245\nFPR for female individuals = 0.1323761665470208\nFNR for female individuals = 0.3088572739788894"
  },
  {
    "objectID": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#bias-measures",
    "href": "posts/Auditing Allocative Bias/Auditing Allocative Bias nb.html#bias-measures",
    "title": "Auditing Allocative Bias",
    "section": "Bias Measures",
    "text": "Bias Measures\nCalibration means that the fraction of predicted people to have an income >= $$$50,000 who actually had an income >=$50,000 is the same across all groups. So we will calculate this metric for both genders. This metric can be calculated by dividing the number of true positives by the total number of predicted positives.\n\nconf_matrix_male = confusion_matrix(y_test[group_test==1],y_hat[group_test==1])\nmale_calibration = confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0][0]/(confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0].sum())\nconf_matrix_female = confusion_matrix(y_test[group_test==2],y_hat[group_test==2])\nfemale_calibration = confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0][0]/(confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0].sum())\n\nprint(\"The fraction of males predicted to have an income >=$50000 who actually had an income of >=$50000 is \" + str(male_calibration.round(4)))\nprint(\"The fraction offe males predicted to have an income >=$50000 who actually had an income of >=$50000 is \" + str(female_calibration.round(4)))\n\n\nThe fraction of males predicted to have an income >=$50000 who actually had an income of >=$50000 is 0.8993\nThe fraction offe males predicted to have an income >=$50000 who actually had an income of >=$50000 is 0.8676\n\n\nI would say that the model is calibrated as the proportions for each group are roughly the same.\nA model satisfies error rate balance if the false positive and false negative rates are equal across groups. Looking at the previously calculated FPR anf FNR, I would say that this model satidfies error rate balance. The FPR for both groups is rounghly 12% and the FNR for both groups is roughly 31%.\nA model satisifes statistical parity if the proportion of individuals classified as having an income >=$50000 is the same for each group. So we compare the total number of predicted positives.\n\nprint(\"The proportion of males classified as having an income greater than $50,000 is \" + str((confusion_matrix(y_test[group_test==1],y_hat[group_test==1])[0].sum())/34377))\nprint(\"The proportion of females classified as having an income greater than $50,000 is \" + str((confusion_matrix(y_test[group_test==2],y_hat[group_test==2])[0].sum())/36491))\n\nThe proportion of males classified as having an income greater than $50,000 is 0.1593798179015039\nThe proportion of females classified as having an income greater than $50,000 is 0.19086898139267217\n\n\nAgain, these values are roughly the same, so I would say that this model satisfies statistical parity.\n\nCompanies like banks or credit card companies could benefit from this model. Banks often give out loans and need to asses how much money is a safe amount to give to their clients. Knowing their clients’ incomes would be very helpful in assesing the risks of pursuing a loan. Credit card companies follow a similar process when determining to accept a clients’ credit card request.\nI think it is interesting that this model gives companies/organizations that don’t usually have access to your income more insight into your financials. Job employers usually don’t have access to your current salary/wage, but now this algorithm will give them more insight and could potentially help them in deciding a new salary offer. Places like country clubs, whos reputation is encompassed by wealth, could use this kind of algorithm.\nBased on my bias audit, I don’t think that my model displays problematic bias. The model satisifies the calibration, error rate and statistical parity tests. So, there is no obvious bias, but as always more investigating is also helpful.\nIn my opion the accuracy of this model is not high enoughly to be deployed. The accuracy of the model is about 80% meaning it predicts the wrong out come 20% of the time. To put this into perspective, this model is incorrect for every 5th person. To help remedy this, I would suggest either collecting more data or reevaluating the parameters/factors involved"
  },
  {
    "objectID": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html",
    "href": "posts/Linear Regression Blog Post/Linear Regression Blog nb.html",
    "title": "Linear Regression",
    "section": "",
    "text": "link to source code"
  },
  {
    "objectID": "posts/Timnit Gebru Blog Post/Timnit Gebru NB.html",
    "href": "posts/Timnit Gebru Blog Post/Timnit Gebru NB.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Mia Tarantola\n\n\nThis blog post will contain two parts. The first portion will be completed prior to our talk and will be based on self guided research on our speaker Timnit Gebru. I will take notes and pose some thoughtful questions that I think will make for a good discussion.\nThe second portion will be completed during/after Gebru’s talk. I will take notes and reflect on the material.\nDr. Timnit Gebru is a successful American computer scientist who focuses on artificial intelligence and algorithmic bias. She has become a well known advocated for diversity in technology and even founded her own community , Black in AI. In December 2020 Gebru left her position as the Ethical Artificial Intelligence Team lead at Google, following pushback fron her as-yet unpuclished paper voicing her concerns on the dangerous biases of large language models.\n\n\n\n\n\nrecording of the talk\nTimnit Gebru’s talk on “Computer Vision in practice: who is benefitting and who is being harmed?” focuses on the ethical ramifications of computer vision technology in our society. Grebru emphasizes the need for critical thought on the possible effects of new technologies on marginalized communities as a top researcher in artifical intelligence and ethics.\nGebru’s talk emphasizes the danger that can result from the use of computer vision technology when it is implemented without adequate consideration of the potential consequences. She discusses how face recognition technology may be used to mistakenly identify people based on their gender and skin color, potentially leading to false charges and arrests. Additionally, predictive policing systems that rely on biased data sets can perpetuate systemic discrimination against communities of color, leading to increased surveillance and targeting. The ethics of data collecting and utilization are also covered by Gebru. She emphasizes that a model’s deployment might still be unethical even if its accuracy is identical among all groups. Images scraped from numerous online sources are frequently included in data sets, often without the subjects’ knowledge. She says that people are unlikely to want their pictures to be used in face recognition software to identify protesters.\nGebru argues that it is essential for individuals who create and use computer vision systems to take into account these possible negative effects and strive toward creating more open and accountable business processes. She stresses the importance of diversity and inclusion in the creation of moral and responsible AI systems. She also encourages businesses and decision-makers to give underrepresented populations’ demands and worries first priority while developing and implementing computer vision technologies.\nGebru’s presentation serves as a reminder of the essential part AI practitioners and researchers must play in influencing the creation and use of computer vision technologies. We can progress toward developing AI systems that are beneficial to every member of society by considering the ethical implications of these systems and including a variety of viewpoints into their development.\nTL;DR In order to create ethical and just predictive modeling systems, we need to prioritize the need for diverse and inclusive development and implementation practices.\n\n\n\n\nHow can we address the issue of biased data sets in computer vision technology, and what strategies can be used to prevent the continuation of systemic discrimination?\nWhat role should companies and policymakers play in regulating the use of computer vision technology, and what policies and practices would you recommend?\nGiven the potential for danger/harm caused by computer vision technology, what ethical considerations should AI researchers keep in mind when designing and deploying these models?\nCan you share any examples of organizations or initiatives that you believe are making progress in addressing the ethical implications of AI, and what we can learn from them?\nHow can we better educate the public about the ethical implications of computer vision technology, and what steps can be taken to increase awareness and engagement on this issue?"
  },
  {
    "objectID": "posts/Unsupervised Learning Blog Post/Unsupervised Learning Nb.html",
    "href": "posts/Unsupervised Learning Blog Post/Unsupervised Learning Nb.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "This is in an optional blog post that focuses on using linear algebra for unsupervised learning. I chose to focus on the first dection : Image compression with Singular value decomposition.\nSVD for a real matrix \\(A \\in R^{mxn}\\) is \\[A = UDV^{T}\\] where \\(D \\in R^{mxn}\\) has non zero entries and where \\(U \\in R^{mxm}\\) and \\(V \\in R^{nxn}\\) are orthogonal matrices.\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nHere we will read in our image, my dog :)\n\nurl = \"https://i.imgur.com/b2Xy4Et.jpeg\"\nimg = read_image(url)\n\nNext, we will convert our RGB image into greyscale. Let’s visualize this:\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nLet’s create a svd construction function. We can call a built in numpy package on our image array. Sigma is a numpy array containing the singular values of our image. We can reconstruct our image from the equation in the introduction by constructing D, the diagonal matrix containg sigma values. Then, we can matrix multiply U,D, and V to reconstruct our image.\nBut, one of the many reasonse we like svd is that we can approximate by using a subset of each matrix. We only need the first k columns of U, the top k singular values in D and the first k rows of V. Assume k is smaller than m and n. Then we can matrix multiply the submatrices for our reconstruction.\n\ndef svd_reconstruct(img,k):\n    U,sigma, V = np.linalg.svd(img)\n    D = np.zeros_like(img,dtype=float)\n    D[:min(img.shape),:min(img.shape)] = np.diag(sigma)    \n    U_ = U[:,:k]\n    D_ = D[:k,:k]\n    V_=V[:k,:]\n    A_ = U_ @ D_ @ V_\n    return A_\n\n\ngrey_img.shape\n\n(480, 360)\n\n\n\n\nNow I will perform an experiment in which I reconstruct my image with different k values. I will increase k until I can no longer differentiate my original image from the reconstructed approximation. I will also determine the ammount of storage.\n\nreconstructions = []\n\nfor i in range (1,7):\n    reconstructions.append((svd_reconstruct(grey_img,12*i),12*i))\n\n\nfig,axs = plt.subplots(2,4, figsize=(20,15))\n\n\ncount = 0\nfor i in range(2):\n    for j in range(3):\n        axs[i][j].imshow(reconstructions[count][0],cmap = \"Greys\")\n        axs[i][j].set_title(str(reconstructions[count][1])+ \" Components, \\n % storage =  \" + str(round(((grey_img.shape[0]+grey_img.shape[1]+1)*reconstructions[count][1])/(grey_img.shape[0]*grey_img.shape[1])*100,2))) \n        axs[i][j].axis(\"off\")\n        count+=1\naxs[0][3].imshow(grey_img,cmap=\"Greys\")\naxs[0][3].axis(\"off\")\naxs[0][3].set_title(\"orig\")\n\naxs[1][3].axis(\"off\")\n\n(0.0, 1.0, 0.0, 1.0)\n\n\n\n\n\n\n\n\nIf an mxn image needs mn pixels(numbers) to represent it, than our original greyscale image needs \\(480 * 360 = 172,800\\) numbers. Now we will consider our new images. We only need the first k columns of U, the top k singular values in D and the first k rows of V. If \\(\\vec{u}\\) is an \\(m x k\\) vector and \\(\\vec{v}\\) is a \\(k x n\\) vector, then the product is a \\(m x n\\) maxtrix with \\(m*n\\) items. However, these items can be stored by just storing \\(\\vec{u_k}\\) and \\(\\vec{v_k}\\). Storing the product \\(D_k \\vec{u_k} \\vec{v_k}\\) can be stored with \\(m+n+k\\) items because we just need to the items in D, k singular values. So, we simply get \\((m+n+1)k\\) items for a reconstructed approximation. And we can now calculate the storage needed for all of our images above.\n\n\n\nThis blog post was pretty cool. I liked working with images and really visualizing the reconstructions in comparision to the original photo. It seems like there was not visible difference between our k = 72 image and our original. Storing this photo only takes 35% of the original storage space!"
  }
]