[
  {
    "objectID": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "href": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "title": "Perceptron Blog Post",
    "section": "",
    "text": "link to perceptron code\n\n\nIn this blog post we will delve into the perceptron algorithm. I have implemented a algorithm that separates binary data. We have also conducted experiments that push the limits of our algorithm and show cases different patterns\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n\n#importing perceptron.py code and updating\nimport perceptron\nfrom perceptron import Perceptron\nimport importlib\nimportlib.reload(perceptron)\n\n<module 'perceptron' from '/Users/mtarantola@middlebury.edu/Downloads/Machine Learning/miatarantola.github.io/posts/Perceptron Blog Post/perceptron.py'>\n\n\n\n\n\nThe following function is used to update the perceptron weights:\n\n\\(\\tilde{w} ^{(t+1)} = \\tilde{w} ^ {(t)} + \\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0) \\tilde{y_i} \\tilde{x_i}\\)\n\nIn order to implement this algorithm, we must follow these steps.\n\\({1.}\\) pick a random index \\(~{i} \\in\\) n. \n\\({2. }\\) predict the label, \\(\\hat{y_i}\\), of our randomly selected data point, \\(\\tilde{x_i}\\).\n\nTo do so we calculate the dot product, \\(\\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle\\) and compare the resulting value to 0.\n\n\nif the result is greater than 0 return 1, otherwise return -1.\n\n\\({3.}\\) compute \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\)\n\nGiven our predicted label, \\(\\hat{y_i}\\) , of 1 or -1 we can multiply by \\(\\tilde{y_i}\\) to check for correctness\n\n\nIf \\(\\hat{y_i} \\tilde{y_i} <0\\) then the signs of our observed and predicted label do not match. If \\(\\hat{y_i} \\tilde{y_i} >0\\) then the signs of our predicted and observed label match\n\nSo, \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\) returns 1 if model predicted incorrectly and \\(\\tilde{y_i} \\tilde{x_i}\\) is added to the pre-existing weights.\nOtherwise the expression returns 0 for a correctly prediction and no update is performed\n\n\n\n\n\n\nnp.random.seed(8674)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = 2,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np1 = Perceptron()\n\n\np1.fit(X1,y1,100000)\n\n\nfig1 = plt.plot(p1.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np1.score(X1,y1)\n\n1.0\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig = draw_line(p1.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nFor this experiment we ran our perceptron algorithm on linearly separable data. We can see that the reuslting line is a good separator as it clearly separates our two labels. It is interesting to see that the accuracy decreases before it increases. Another result that suggests our separator is a good fit is that our algorithm stops before the maximum number of iterations allowed\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\np_features = 3\n\nX2, y2 = make_blobs(n_samples = 100,n_features=2, centers=2)\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np2 = Perceptron()\n\n\np2.fit(X2,y2,1000)\n\n\nfig3 = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X2,y2)\n\n0.5\n\n\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nThis experiment displays the use of our perceptron algorithm on nonlinearly separable data. As shown above, the resulting line is not a good separater. In fact, it isn’t separating any data. We can also see the inaccuracy by tracking the accuracy over iterations. We can see that the algorihtm is not converging to one set of weights and thus has not found an optimizer.\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\n\nX3, y3 = make_blobs(n_samples = 100,n_features=6, centers=2)\n\n\np3 = Perceptron()\n\n\np3.fit(X3,y3,1000)\n\n\nfig4 = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X3,y3)\n\n1.0\n\n\nYes, I do believe that my data is linearly separable because my perceptron’s accuracy converges to 1 after 18 iterations. This means that my perceptron reached 100% accuracy and did not complete upon reaching that maximum number of iterations.\n\n\n\n\nThe time complexity for a single iteration of the perceptron algorithm is O(p) because predicting the label of a single data point requires calculating the dot product of x and w. Thus, np iterates over the p features of x to calulate the dot product. All other steps of this equation invlove simple multiplication or addition which are O(1)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An blog post illustrating the key techniques of gradient descent\n\n\n\n\n\n\nMar 9, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn blog post practing machine learning techniques on real life data\n\n\n\n\n\n\nMar 9, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn blog post illustrating course material on perceptrons\n\n\n\n\n\n\nFeb 19, 2023\n\n\nMia Tarantola\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "We are looking at the use of gradient descent for optimization and the logistic loss problem. In this assignment I implement gradient descent for logistic regression, implement stochastic gradient descent and perform several experiments that test the limits of the algorithms\n\n\n\n\nimport importlib\nimport gradient_descent\nfrom gradient_descent import LogisticRegression \n\n%load_ext autoreload\n%autoreload 2\n\nfrom sklearn.datasets import make_blobs, make_circles\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#stochastic-descent",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#stochastic-descent",
    "title": "Gradient Descent",
    "section": "Stochastic Descent",
    "text": "Stochastic Descent\n\nnp.random.seed(8674)\n\nX2, y2 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR2 = LogisticRegression()\nLR2.fit_stochastic(X2, y2, alpha = 0.1, max_epochs = 1000, batch_size = 15)\n\nThe Stochastic fit function beginns similiarly to the original gradient descent function: > - create X_ > - generate random weight vector of size features+1 > - set previous loss = infinity\nThen we have to iterate thrugh the following for the number of max epochs or until the loss converges:\n\n\nshuffle the points randomly\npick the first random k points and update the weights vector using the stochastic gradient\npick the next set of random points and repeat\nupdate the loss and score history\nreshuffle the points randomly and iterate again\n\n\n\n\nthe gradient function is the same as the original one, we are just inputting a subset of X_ and y_ into the function repeatedly\n\n\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig2 = draw_line(LR2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nprint(LR2.loss_history[-3])\nprint(LR2.loss_history[-2])\nprint(LR2.loss_history[-1])\nprint(LR2.score_history[-1])\n\n0.0033704921863074765\n0.003368276616639715\n0.0033660810616696095\n1.0\n\n\nThis is a good separator as our loss is close to 0 and our score is close to 1"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#comparing-methods",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#comparing-methods",
    "title": "Gradient Descent",
    "section": "Comparing methods",
    "text": "Comparing methods\n\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  max_epochs = 1000, \n             \n                  batch_size = 10, \n                  alpha = .05) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (a=0.05)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X2, y2, \n                  max_epochs = 1000, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient a = .1\")\n\nLR = LogisticRegression()\nLR.fit(X2, y2, alpha = .05, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nFor these settings, we can see that the stochastic methods are performing much better than the regular gradient descent method. All three methods have a smooth decline in loss, but the stochastic gradient seems to have had the fastest decline. I believe the the gradient methods would need more epochs to find a better solution."
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-alpha-is-too-large",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-alpha-is-too-large",
    "title": "Gradient Descent",
    "section": "Experiment: alpha is too large",
    "text": "Experiment: alpha is too large\n\nnp.random.seed(4001)\n\nn = 100\np_features = 3\n\nX3, y3 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n#alpha is too large\nLR3 = LogisticRegression()\nLR3.fit(X3, y3, alpha = 200, max_epochs = 100)\n\n#alpha is a normal value\nLR5 = LogisticRegression()\nLR5.fit(X3, y3, alpha = .01, max_epochs = 1000)\n\n\n\n#alpha = 200\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR3.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\nLR3.loss_history\nLR3.w\n\narray([19.43660085, 18.2651358 , -4.47025268])\n\n\n\n\n\n\n#alpha = .01\nfig3 = plt.scatter(X3[:,0], X3[:,1], c = y3)\nfig3 = draw_line(LR5.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nnum_steps1 = len(LR3.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR3.loss_history, label = \"(a=200)\")\n\nnum_steps2 = len(LR5.loss_history)\nplt.plot(np.arange(num_steps2) + 1, LR5.loss_history, label = \"a =.01\")\n\n\n\n\nlegend = plt.legend() \n\n\n\n\nWe can see that both alphas result in good separators. But, we have discovered one of the caveats of logistic regression. It can combat alphas that are too large by increasing all of the weights. So, for data that is linearly separable the model performs as good as you want it to. Hence, we will now investigate non-linearly separable data\n\nNon linearly separable\n\nnp.random.seed(8680)\n\nn = 100\np_features = 4\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1,-1),(1,1)])\n\n\n\nfig1 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR6 = LogisticRegression()\nLR6.fit(X4, y4, alpha = 100, max_epochs = 1000)\n\nLR7 = LogisticRegression()\nLR7.fit(X4, y4, alpha = 1, max_epochs = 1000)\n\n\n\nfig3 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nfig3 = draw_line(LR6.w, -2, 2)\n\n\n\n\n\nfig3 = plt.scatter(X4[:,0], X4[:,1], c = y4)\nfig3 = draw_line(LR7.w, -2, 2)\n\n\n\n\n\n\nnum_steps1 = len(LR6.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR6.loss_history, label = \"(a=100)\")\n\n\nnum_steps1 = len(LR7.loss_history)\nplt.plot(np.arange(num_steps1) + 1, LR7.loss_history, label = \"(a=1)\")\n\n\nlegend = plt.legend() \n\n\n\n\nHere we can see that our model performs fairly well at a smaller alpha (1) as the line mostly separates the data. However, for a larger alpha the algorithm does not work well. It is unable to converge and thus results in a bad separater and the loss bounces around"
  },
  {
    "objectID": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-batch-size",
    "href": "posts/Gradient Descent Blog Post/Gradient_Descent_blog.html#experiment-batch-size",
    "title": "Gradient Descent",
    "section": "Experiment batch size",
    "text": "Experiment batch size\n\nnp.random.seed(8680)\n\nn = 100\np_features = 4\n\nX4, y4 = make_blobs(n_samples = 100, n_features = p_features-1,centers=[(-1.7,-1.7),(1.7,1.7)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X4, y4, \n                  max_epochs = 10000, \n                  batch_size = 8, \n                  alpha = .1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch_size=8)\")\n\nLR = LogisticRegression()\nLR.fit_stochastic(X4, y4, \n                  max_epochs = 10000, \n                  batch_size = 80, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (batch_size=80)\")\n\n\nplt.loglog()\n\nlegend = plt.legend() \n\n\n\n\nFor this experiment I compared a stochastic batch size of 8 to a batch size of 80. We can see that a smaller batch size allows the algorithm to converge must faster. Increasing the batch size by a factor of 8 almost increased the number of needed epochs by a factor of 10"
  },
  {
    "objectID": "posts/Classifying Palmer Penguins/Penguin Blog Post.html",
    "href": "posts/Classifying Palmer Penguins/Penguin Blog Post.html",
    "title": "Penguin Blog Post",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\",\"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head() #looking at data\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n\n\n\n\ny_train.mean()\n\n0.95703125\n\n\n\n\n\nThe purpose of this blog post is to practice implemeting machine learning classifiers on real life data! We are looking at the plamer penguins data set collected by Dr. Kristen Gorman. Here we analyze different attributes to use in our species prediction for penguins\n\n\n\npalmer_penguins.png\n\n\n\n\n\n\nsns.relplot(data = train, x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\", hue = \"Species\", size = \"Body Mass (g)\")\n\n<seaborn.axisgrid.FacetGrid at 0x7f780c3ac9d0>\n\n\n\n\n\nThe graph explores the realtionship between species, Flipper Length and Culmen Length. We can see a clear divide between the species. The Gentoo penguins seem to have the longeth flipper and culmen length and are the heaviest. Chinstrap penguins appear to have a longer culmen length but a shorter flipper length. Adelie Penguins have the shortest Flipper and Culmen length. Despite having a longer culmen length, the chinstrap penguins are about the same weight as the Adelies.\n\ntrain.groupby(\"Species\").agg(\"mean\")\n\n/var/folders/k6/ddn01bfs47n_n5b4t6fb88q80000gp/T/ipykernel_58878/1693257315.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  train.groupby(\"Species\").agg(\"mean\")\n\n\n\n\n\n\n  \n    \n      \n      Sample Number\n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n    \n    \n      Species\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Adelie Penguin (Pygoscelis adeliae)\n      73.440678\n      38.710256\n      18.365812\n      189.965812\n      3667.094017\n      8.855229\n      -25.825165\n    \n    \n      Chinstrap penguin (Pygoscelis antarctica)\n      32.696429\n      48.719643\n      18.442857\n      195.464286\n      3717.857143\n      9.338671\n      -24.542886\n    \n    \n      Gentoo penguin (Pygoscelis papua)\n      61.930693\n      47.757000\n      15.035000\n      217.650000\n      5119.500000\n      8.240438\n      -26.168937\n    \n  \n\n\n\n\nThis tables looks at the quantitative variables averaged by species. It is interesting to see that the Adelie penguins have a deeper culmen, yet are significantly lighter than the Gentoo penguins. In general a larger flipper length correlates to a heavier body mass. However, the penguins with a smaller culmen depth have a lighter body mass.\n\n\n\nFirst we must check the base rate of our data set - If we were to guess one species for all samples, what would the highest accuracy be? To do this we must calculate the number of each species and compare this to the total number of penguins\n\nnum_penguins = y_train.shape[0]\nspecies1 = np.count_nonzero(y_train == 0)\nspecies2 = np.count_nonzero(y_train == 1)\nspecies3 = np.count_nonzero(y_train == 2)\n\nmax([species1,species2,species3])/num_penguins\n\n0.4140625\n\n\nHere we took the spcies with the largest number of samples and divided by the total number of penguins. This gives us our base rate. If we guessed one species for all of the penguins we would be correct at most 41% of the time\n\n\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nimport numpy as np \n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Sex\",\"Clutch Completion\",\"Island\",\"Region\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\"]\n\nbest_col= []\nbest_score = -np.inf\n\nbest_gamma=np.inf\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    for gammas in 10**(np.arange(-4, 4,dtype=float)):\n        clf = SVC(gamma = gammas) #kernel=\"linear\",probability=True)\n        scores = cross_val_score(clf, X_train[cols], y_train, cv=5)\n        if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_col= cols\n            best_gamma = gammas\nprint(best_col, best_score, best_gamma)\n        \n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.984389140271493 0.1\n\n\nThe method above chooses the best penguin attributes to use for our model as well as the best gamma, which determines the complexity of the model. To do so we iterate through the different combinations of attributes incluiding 1 qualitative and 2 quantitative variables. We then iterate through a series of gammas and analyze the accuracy of an SVC model using cross validation. The best attributes, score and gamma are recorded for testing\n\n\nbest_model = SVC(gamma = best_gamma)\n\nHere we initialize and train a final model using our initial dataset, and optimized parameters that we just discovered\n\nbest_model.fit(X_train[sorted(best_col, reverse = True)],y_train)\n\nSVC(gamma=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(gamma=0.1)\n\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nbest_model.score(X_test[sorted(best_col,reverse=True)], y_test)\n\n0.9558823529411765\n\n\nWe can see that the final model predicts a penguins species, given their island, culmen length and culmen depth with 95% accuracy. This is definitely better than our base rate, However, we will now experiment with a different method to see if we can increase our results\n\n\n\n\n\nfrom itertools import combinations\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nimport numpy as np \n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Sex\",\"Clutch Completion\",\"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\"]\n\nbest_col= []\nbest_score = -np.inf\n\nmaximum=np.inf\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    for depth in range(2,25):\n        clf = DecisionTreeClassifier(max_depth = depth)\n        scores = cross_val_score(clf, X_train[cols], y_train, cv=4)\n        if scores.mean()>best_score:\n            best_score = scores.mean()\n            best_col= cols\n            maximum = depth\nprint(best_col, best_score, maximum)\n\n        \n\n['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)'] 0.984375 4\n\n\nSimilar to the method above, we begin by iterating through all combinations of attributes. Next we use cross validation to determine the best value for maximum depth. We keep track of the best attributes, score and depth.\n\nmodel = DecisionTreeClassifier(max_depth = maximum)\nmodel.fit(X_train[sorted(best_col)],y_train)\nmodel.score(X_test[sorted(best_col)], y_test).round(3)\n\n0.985\n\n\nWe trained a final model with our optimal parameters and tested it our test data. The final model predicted the correct penguin species 98.5% of the time. This is better than the base rate as well as the SVC model. We will now dive further into the accruacy of this model\n\nWe will look at a confusion matrix to see what our accuracy loss is originating. As we can see below, the model preformed accrurate on all but one penguin. This penguin was species1 however our model predicted that it was a species 2 penguin.\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict(X_test[sorted(best_col)])\n\nconfusion_matrix(y_test, y_pred)\n\narray([[32,  1,  0],\n       [ 0, 12,  0],\n       [ 0,  0, 23]])\n\n\n\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n \n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\n\nplot_regions(model,X_train[sorted(best_col)],y_train)\n\n\n\n\nThe plots above visualize our decision regions for the decision tree model. We can see that our model does a pretty good job at separating the different species; however, the middle plot has some samples right on the edge\n\n\n\n\nThis blog post allowed me to practice a machine learning workflow and get hands on experience implementing a classifier with real data. I really enjoyed exploring the different classification models and optimization parameters. I also enjoyed working with real life data, I can really see how this will help me in my future!"
  }
]