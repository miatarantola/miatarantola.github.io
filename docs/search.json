[
  {
    "objectID": "Perceptron Blog Post/Perceptron Blog.html",
    "href": "Perceptron Blog Post/Perceptron Blog.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "link to perceptron repository [link]_\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n\n#importing perceptron.py code and updating\nimport perceptron\nfrom perceptron import Perceptron\nimport importlib\nimportlib.reload(perceptron)\n\n<module 'perceptron' from '/Users/mtarantola@middlebury.edu/Downloads/Machine Learning/perceptron.py'>\n\n\n\n\n\nThe following function is used to update the perceptron weights:\n\n\\(\\tilde{w} ^{(t+1)} = \\tilde{w} ^ {(t)} + \\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0) \\tilde{y_i} \\tilde{x_i}\\)\n\nIn order to implement this algorithm, we must follow these steps.\n\\({1.}\\) pick a random index \\(~{i} \\in\\) n. \n\\({2. }\\) predict the label, \\(\\hat{y_i}\\), of our randomly selected data point, \\(\\tilde{x_i}\\).\n\nTo do so we calculate the dot product, \\(\\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle\\) and compare the resulting value to 0.\n\n\nif the result is greater than 0 return 1, otherwise return -1.\n\n\\({3.}\\) compute \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\)\n\nGiven our predicted label, \\(\\hat{y_i}\\) , of 1 or -1 we can multiply by \\(\\tilde{y_i}\\) to check for correctness\n\n\nIf \\(\\hat{y_i} \\tilde{y_i} <0\\) then the signs of our observed and predicted label do not match. If \\(\\hat{y_i} \\tilde{y_i} >0\\) then the signs of our predicted and observed label match\n\nSo, \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\) returns 1 if model predicted incorrectly and \\(\\tilde{y_i} \\tilde{x_i}\\) is added to the pre-existing weights.\nOtherwise the expression returns 0 for a correctly prediction and no update is performed\n\n\n\n\n\n\nnp.random.seed(8674)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = 2,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np1 = Perceptron()\n\n\np1.fit(X1,y1,100000)\n\n\nfig1 = plt.plot(p1.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np1.score(X1,y1)\n\n1.0\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig = draw_line(p1.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\np_features = 3\n\nX2, y2 = make_blobs(n_samples = 100,n_features=2, centers=2)\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np2 = Perceptron()\n\n\np2.fit(X2,y2,1000)\n\n\nfig3 = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X2,y2)\n\n0.5\n\n\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\n\nX3, y3 = make_blobs(n_samples = 100,n_features=6, centers=2)\n\n\np3 = Perceptron()\n\n\np3.fit(X3,y3,1000)\n\n\nfig4 = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X3,y3)\n\n1.0\n\n\nYes, I do believe that my data is linearly separable because my perceptron’s accuracy converges to 1 after 18 iterations. This means that my perceptron reached 100% accuracy and did not complete upon reaching that maximum number of iterations.\n\n\n\n\nThe time complexity for a single iteration of the perceptron algorithm is O(p) because predicting the label of a single data point requires calculating the dot product of x and w. Thus, np iterates over the p features of x to calulate the dot product. All other steps of this equation invlove simple multiplication or addition which are O(1)."
  },
  {
    "objectID": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "href": "posts/Perceptron Blog Post/Perceptron Blog.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "link to perceptron code\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n\n#importing perceptron.py code and updating\nimport perceptron\nfrom perceptron import Perceptron\nimport importlib\nimportlib.reload(perceptron)\n\n<module 'perceptron' from '/Users/mtarantola@middlebury.edu/Downloads/Machine Learning/perceptron.py'>\n\n\n\n\n\nThe following function is used to update the perceptron weights:\n\n\\(\\tilde{w} ^{(t+1)} = \\tilde{w} ^ {(t)} + \\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0) \\tilde{y_i} \\tilde{x_i}\\)\n\nIn order to implement this algorithm, we must follow these steps.\n\\({1.}\\) pick a random index \\(~{i} \\in\\) n. \n\\({2. }\\) predict the label, \\(\\hat{y_i}\\), of our randomly selected data point, \\(\\tilde{x_i}\\).\n\nTo do so we calculate the dot product, \\(\\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle\\) and compare the resulting value to 0.\n\n\nif the result is greater than 0 return 1, otherwise return -1.\n\n\\({3.}\\) compute \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\)\n\nGiven our predicted label, \\(\\hat{y_i}\\) , of 1 or -1 we can multiply by \\(\\tilde{y_i}\\) to check for correctness\n\n\nIf \\(\\hat{y_i} \\tilde{y_i} <0\\) then the signs of our observed and predicted label do not match. If \\(\\hat{y_i} \\tilde{y_i} >0\\) then the signs of our predicted and observed label match\n\nSo, \\(\\mathbb{1}(\\tilde{y_i} \\langle \\tilde{w}^{(t)}, \\tilde{x_i} \\rangle < 0)\\) returns 1 if model predicted incorrectly and \\(\\tilde{y_i} \\tilde{x_i}\\) is added to the pre-existing weights.\nOtherwise the expression returns 0 for a correctly prediction and no update is performed\n\n\n\n\n\n\nnp.random.seed(8674)\n\nn = 100\np_features = 3\n\nX1, y1 = make_blobs(n_samples = 100, n_features = 2,centers=[(-1.7,-1.7),(1.7,1.7)])\n\nfig1 = plt.scatter(X1[:,0], X1[:,1], c = y1)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np1 = Perceptron()\n\n\np1.fit(X1,y1,100000)\n\n\nfig1 = plt.plot(p1.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np1.score(X1,y1)\n\n1.0\n\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X1[:,0], X1[:,1], c = y1)\nfig = draw_line(p1.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\np_features = 3\n\nX2, y2 = make_blobs(n_samples = 100,n_features=2, centers=2)\n\nfig2 = plt.scatter(X2[:,0], X2[:,1], c = y2)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\np2 = Perceptron()\n\n\np2.fit(X2,y2,1000)\n\n\nfig3 = plt.plot(p2.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np2.score(X2,y2)\n\n0.5\n\n\n\nfig = plt.scatter(X2[:,0], X2[:,1], c = y2)\nfig = draw_line(p2.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\n\n\nnp.random.seed(7810) #7810\n\nn = 100\n\nX3, y3 = make_blobs(n_samples = 100,n_features=6, centers=2)\n\n\np3 = Perceptron()\n\n\np3.fit(X3,y3,1000)\n\n\nfig4 = plt.plot(p3.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\n\np3.score(X3,y3)\n\n1.0\n\n\nYes, I do believe that my data is linearly separable because my perceptron’s accuracy converges to 1 after 18 iterations. This means that my perceptron reached 100% accuracy and did not complete upon reaching that maximum number of iterations.\n\n\n\n\nThe time complexity for a single iteration of the perceptron algorithm is O(p) because predicting the label of a single data point requires calculating the dot product of x and w. Thus, np iterates over the p features of x to calulate the dot product. All other steps of this equation invlove simple multiplication or addition which are O(1)."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "An example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]